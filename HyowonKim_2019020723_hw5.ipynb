{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyowonKim_2019020723_hw5.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "display_name": "Python [conda env:tensorflow]",
      "language": "python",
      "name": "conda-env-tensorflow-py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyUY1ZCL9w5N",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"text-align: right\"> __ Provided on May 27 and due on June 17__ [BRI516, Spring/2019] </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCIoyyp-9w5P",
        "colab_type": "text"
      },
      "source": [
        "For homework in general:\n",
        "* Install `Anaconda` and create an environment with `NumPy`, `Pandas`, `Matplotlib`, `scikit-learn` in Python 3.5 \n",
        "* Please upload your jupyter-notebook file for homework to `Blackboard`\n",
        "* Please visualize the results as much as possible and discuss your results at least one line of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61HYus1M9w5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Disable warning message\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS75YBtX9w5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## import library & module\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import struct\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbNvH1aL9w5T",
        "colab_type": "text"
      },
      "source": [
        "#### [Hw05] Implementing a Multilayer Neural Networks using TensorFlow (TF) 1.x (Ch. 13)\n",
        "\n",
        "\n",
        "##### (1) Data preparation\n",
        "    (1a) Load the train dataset and test dataset of the MNIST \n",
        "    \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1bxr6BG9w5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "e254b55c-fa23-44f5-9c9f-b235aded666b"
      },
      "source": [
        "## import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets(\"./data/mnist/\", validation_size=5000)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 12:13:29.724256 139956813535104 deprecation.py:323] From <ipython-input-3-f6494e31dbf4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0617 12:13:29.725959 139956813535104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0617 12:13:29.727774 139956813535104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 12:13:30.010972 139956813535104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0617 12:13:30.061809 139956813535104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kazsjSu9w5X",
        "colab_type": "text"
      },
      "source": [
        "    (1b) Set the first 55,000 samples in the training dataset as training data, the remaining 5,000 samples in the training dataset as validation data, and 10,000 samples in the test dataset as test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvabCxXH9w5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "73ef2278-a759-41ba-84f6-82e27020562d"
      },
      "source": [
        "## Set the first 55,000 samples in the training dataset as training data\n",
        "X_train = mnist_data.train.images\n",
        "y_train = mnist_data.train.labels\n",
        "print('training data\\n rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
        "\n",
        "# the remaining 5,000 samples in the training dataset as validation data\n",
        "X_valid = mnist_data.validation.images\n",
        "y_valid = mnist_data.validation.labels\n",
        "print('validation data\\n rows: %d, columns: %d' % (X_valid.shape[0], X_valid.shape[1]))\n",
        "\n",
        "# 10,000 samples in the test dataset as test data\n",
        "X_test = mnist_data.test.images\n",
        "y_test = mnist_data.test.labels\n",
        "print('test data\\n rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data\n",
            " rows: 55000, columns: 784\n",
            "validation data\n",
            " rows: 5000, columns: 784\n",
            "test data\n",
            " rows: 10000, columns: 784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPLyUWan9w5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "b972e004-1ef7-45ee-a3fd-40e7b0055d1b"
      },
      "source": [
        "# check\n",
        "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
        "ax = ax.flatten()\n",
        "for i in range(10):\n",
        "    img = X_train[y_train == i][0].reshape(28, 28)\n",
        "    ax[i].imshow(img, cmap='Greys')\n",
        "    \n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADRCAYAAACZ6CZ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyI7sU2U9w5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8cbc90a8-9eb6-4679-f377-279fb694decb"
      },
      "source": [
        "## mean centering and normalization:\n",
        "mean_vals = np.mean(X_train, axis=0)\n",
        "std_val = np.std(X_train)\n",
        "\n",
        "X_train_centered = (X_train - mean_vals)/std_val\n",
        "X_valid_centered = (X_valid - mean_vals)/std_val\n",
        "X_test_centered = (X_test - mean_vals)/std_val\n",
        "\n",
        "del X_train, X_test, X_valid\n",
        "\n",
        "print(X_train_centered.shape, y_train.shape)\n",
        "print(X_valid_centered.shape, y_valid.shape)\n",
        "print(X_test_centered.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55000, 784) (55000,)\n",
            "(5000, 784) (5000,)\n",
            "(10000, 784) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQNen0Zo9w5e",
        "colab_type": "text"
      },
      "source": [
        "##### (2) Multilayer perceptron (MLP) using the low-level API of TF (e.g., p. 429)\n",
        "    (2a) Implement the multilayer perceptron (MLP) with two hidden layers for classification, and evaluate performance in the following scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZbBomL9w5f",
        "colab_type": "text"
      },
      "source": [
        "        (2a-1) for a few choices of hidden nodes per hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJHje54X9w5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_batch_generator(X, y, batch_size=1, shuffle=False):\n",
        "    X_copy = np.array(X)\n",
        "    y_copy = np.array(y)\n",
        "    \n",
        "    if shuffle:\n",
        "        data = np.column_stack((X_copy, y_copy))\n",
        "        np.random.shuffle(data)\n",
        "        X_copy = data[:, :-1]\n",
        "        y_copy = data[:, -1].astype(int)\n",
        "        \n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYbKjhVg9w5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 미리 코드상에는 2a), 2b) 2c) 구현 다 해놓은 후 각각을 값을 줘서 실행할 수 있는 코드로 설정\n",
        "class MLP(object):    \n",
        "    def __init__(self, n_features, epochs=10, learning_rate=0.001, l2= 0, n_hidden=32, batch_size=1, shuffle=False, random_seed=None, activation_fn='sigmoid'):\n",
        "        np.random.seed(random_seed)\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.shuffle = shuffle\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_features = n_features\n",
        "        self.valid_acc_ = 0.0\n",
        "        self.train_loss_ = 0.0\n",
        "        self.output_ = 0.0\n",
        "        self.l2 = l2\n",
        "        self.batch_size = batch_size\n",
        "        if activation_fn == 'sigmod':\n",
        "            self.activation = tf.nn.sigmoid\n",
        "        elif activation_fn == 'tanh':\n",
        "            self.activation = tf.nn.tanh\n",
        "        elif activation_fn == 'relu':\n",
        "            self.activation = tf.nn.relu\n",
        "        else: # default\n",
        "            self.activation = tf.nn.sigmoid\n",
        "            \n",
        "        self.test=0.\n",
        "                \n",
        "        g = tf.Graph()\n",
        "        with g.as_default():\n",
        "            ## set random-seed:\n",
        "            tf.set_random_seed(random_seed)\n",
        "            \n",
        "            ## build the network:\n",
        "            self.build()\n",
        "            \n",
        "            ## initializer\n",
        "            self.init_op = tf.global_variables_initializer()\n",
        "            \n",
        "        ## create a session\n",
        "        self.sess = tf.Session(graph=g)\n",
        "    \n",
        "    def build(self):\n",
        "        n_classes = 10\n",
        "        \n",
        "        ## Placeholders for X and y:\n",
        "        tf_x = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name='tf_x')\n",
        "        tf_y = tf.placeholder(dtype=tf.int32, shape=None, name='tf_y')\n",
        "        \n",
        "        ## One-hot encoding:\n",
        "        y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n",
        "        \n",
        "        ## Store layers weight & bias\n",
        "        weights = {\n",
        "        'h1': tf.Variable(tf.random_normal([self.n_features, self.n_hidden])),\n",
        "        'h2': tf.Variable(tf.random_normal([self.n_hidden, self.n_hidden])),\n",
        "        'out': tf.Variable(tf.random_normal([self.n_hidden, n_classes]))\n",
        "        }\n",
        "        biases = {\n",
        "            'b1': tf.Variable(tf.random_normal([self.n_hidden])),\n",
        "            'b2': tf.Variable(tf.random_normal([self.n_hidden])),\n",
        "            'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "        }\n",
        "            \n",
        "        ## 1st layer(Hidden#1)\n",
        "        layer_1 = self.activation(tf.add(tf.matmul(tf_x, weights['h1']), biases['b1']))\n",
        "        \n",
        "        ## 2nd layer(Hidden#2)\n",
        "        layer_2 = self.activation(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
        "        \n",
        "        ## output\n",
        "        logits = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "        self.output_ = logits\n",
        "        \n",
        "        ## Prediction\n",
        "        predictions = {\n",
        "            #'classes' : tf.argmax(logits, axis=1, name='predicted_classes'),\n",
        "            'probabilities' : tf.nn.softmax(logits, name='softmax_tensor'),\n",
        "            'labels' : tf.cast(tf.argmax(logits, axis=1), tf.int32, name='labels')\n",
        "        }\n",
        "        \n",
        "        ## Loss Function and Optimization\n",
        "        regularizer = (self.l2 / 2) * (tf.reduce_mean(tf.square(weights['h1'])) + tf.reduce_mean(tf.square(weights['h2'])))\n",
        "        cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_onehot),name='cross_entropy_loss')\n",
        "        cross_entropy_loss_l2 = tf.add(cross_entropy_loss, regularizer, name='cross_entropy_loss_l2')\n",
        "        \n",
        "        ## Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
        "        optimizer = optimizer.minimize(cross_entropy_loss_l2, name='train_op')\n",
        "        \n",
        "        ## Finding accuracy\n",
        "        correct_predictions = tf.equal(predictions['labels'], tf_y, name='correct_preds')\n",
        "        \n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
        "        \n",
        "    def train(self, training_set, validation_set=None, initialize=True):\n",
        "        ## initialize variables\n",
        "        if initialize:\n",
        "            self.sess.run(self.init_op)\n",
        "        \n",
        "        self.train_cost_ = []\n",
        "        X_data = np.array(training_set[0])\n",
        "        y_data = np.array(training_set[1])\n",
        "        \n",
        "        if self.batch_size > 1:\n",
        "            self.shuffle = True\n",
        "                \n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            batch_gen = create_batch_generator(X_train_centered, y_train, batch_size=self.batch_size, shuffle=self.shuffle)\n",
        "            \n",
        "            avg_loss = 0.0\n",
        "            for i, (batch_x,batch_y) in enumerate(batch_gen):\n",
        "                feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y}\n",
        "                loss, _ = self.sess.run(['cross_entropy_loss:0', 'train_op'], feed_dict=feed)\n",
        "                avg_loss += loss\n",
        "                \n",
        "            self.train_loss_ = avg_loss\n",
        "            print('Epoch %02d: Training Avg. Loss: ' '%7.3f' % (epoch, avg_loss), end=' ')\n",
        "            \n",
        "            if validation_set is not None:\n",
        "                feed = {'tf_x:0': validation_set[0], 'tf_y:0': validation_set[1]}\n",
        "                valid_acc = self.sess.run('accuracy:0', feed_dict=feed)\n",
        "                self.valid_acc_ = valid_acc\n",
        "                print('Validation Acc: %7.3f' % self.valid_acc_)\n",
        "            else:\n",
        "                self.valid_acc_ = 0.0\n",
        "                print()\n",
        "                \n",
        "    def predict(self, X_test, return_proba = False):\n",
        "        feed = {'tf_x:0': X_test}\n",
        "        if return_proba:\n",
        "            return self.sess.run('probabilities:0', feed_dict=feed)\n",
        "        else:\n",
        "            return self.sess.run('labels:0', feed_dict=feed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7agvBcov9w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## global varaibles : n_features\n",
        "n_features = X_train_centered.shape[1]\n",
        "n_epochs = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgBtpOZC9w5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "af000ea8-725f-4a69-d321-4d30f1762f24"
      },
      "source": [
        "## a few choices of hidden nodes per hidden layer\n",
        "hidden_nodes = [64, 256, 512, 1024]\n",
        "val_accs_0 = []\n",
        "cost_accs_0 = []\n",
        "pred_accs_0 = []\n",
        "for n_hidden in hidden_nodes:\n",
        "    mlp = MLP(n_hidden=n_hidden, n_features=n_features, random_seed=123, epochs=n_epochs)\n",
        "    \n",
        "    print('======== hidden size : %d ========' % n_hidden)\n",
        "    mlp.train(training_set=(X_train_centered, y_train), validation_set=(X_valid_centered, y_valid))\n",
        "    \n",
        "    val_accs_0.append(mlp.valid_acc_)\n",
        "    cost_accs_0.append(mlp.train_loss_)\n",
        "    pred_accs_0.append(mlp.predict(X_test_centered))\n",
        "    \n",
        "min_index = cost_accs_0.index(min(cost_accs_0))\n",
        "best_hidden = hidden_nodes[min_index]\n",
        "preds0 = pred_accs_0[min_index]\n",
        "print('\\n** best hidden node : %d' % best_hidden)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== hidden size : 64 ========\n",
            "Epoch 01: Training Avg. Loss: 93909.072 Validation Acc:   0.665\n",
            "Epoch 02: Training Avg. Loss: 48518.230 Validation Acc:   0.751\n",
            "Epoch 03: Training Avg. Loss: 39219.906 Validation Acc:   0.790\n",
            "Epoch 04: Training Avg. Loss: 34440.583 Validation Acc:   0.807\n",
            "======== hidden size : 256 ========\n",
            "Epoch 01: Training Avg. Loss: 137716.976 Validation Acc:   0.728\n",
            "Epoch 02: Training Avg. Loss: 53930.696 Validation Acc:   0.787\n",
            "Epoch 03: Training Avg. Loss: 39378.400 Validation Acc:   0.808\n",
            "Epoch 04: Training Avg. Loss: 31823.244 Validation Acc:   0.823\n",
            "======== hidden size : 512 ========\n",
            "Epoch 01: Training Avg. Loss: 162072.013 Validation Acc:   0.784\n",
            "Epoch 02: Training Avg. Loss: 60281.029 Validation Acc:   0.816\n",
            "Epoch 03: Training Avg. Loss: 38879.645 Validation Acc:   0.832\n",
            "Epoch 04: Training Avg. Loss: 27779.579 Validation Acc:   0.838\n",
            "======== hidden size : 1024 ========\n",
            "Epoch 01: Training Avg. Loss: 205268.568 Validation Acc:   0.808\n",
            "Epoch 02: Training Avg. Loss: 66040.968 Validation Acc:   0.830\n",
            "Epoch 03: Training Avg. Loss: 32322.833 Validation Acc:   0.836\n",
            "Epoch 04: Training Avg. Loss: 17401.875 Validation Acc:   0.837\n",
            "\n",
            "** best hidden node : 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOydx6Az9w5m",
        "colab_type": "text"
      },
      "source": [
        "        (2a-2) for a few choices of activation functions (i.e., sigmoid, tanh, and ReLU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO-BEyaF9w5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "513677b0-97e5-4830-b52a-1e4858def422"
      },
      "source": [
        "## a few choices of activation functions\n",
        "activations = ['sigmoid', 'tanh', 'relu']\n",
        "val_accs_1 = []\n",
        "cost_accs_1 = []\n",
        "pred_accs_1 = []\n",
        "for activation_fn in activations:\n",
        "    mlp_act = MLP(n_hidden=best_hidden, activation_fn=activation_fn, random_seed=123, epochs=n_epochs, n_features=n_features)\n",
        "    \n",
        "    print('======== activation function : %s ========' % activation_fn)\n",
        "    mlp_act.train(training_set=(X_train_centered, y_train), validation_set=(X_valid_centered, y_valid))\n",
        "    \n",
        "    val_accs_1.append(mlp_act.valid_acc_)\n",
        "    cost_accs_1.append(mlp_act.train_loss_)\n",
        "    pred_accs_1.append(mlp_act.predict(X_test_centered))\n",
        "    \n",
        "min_index = cost_accs_1.index(min(cost_accs_1))\n",
        "best_activation = activations[min_index]\n",
        "preds1 = pred_accs_1[min_index]\n",
        "print('\\n** best activation function : %s' % best_activation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== activation function : sigmoid ========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRR_b-uk9w5o",
        "colab_type": "text"
      },
      "source": [
        "      (2b) Add the L2-norm regularization of weights to (2a) and evalute performance for a few choices of L2-norm regularization parameters. For this, please pick the best-performing model from (2a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2fSdZnZ9w5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## a few choices of L2-norm regularization parameters\n",
        "l2_weight_decays = [1e-4, 1e-3, 1e-2, 1e-1]\n",
        "val_accs_2 = []\n",
        "cost_accs_2 = []\n",
        "pred_accs_2 = []\n",
        "for l2_weight_decay in l2_weight_decays:\n",
        "    mlp_l2 = MLP(n_hidden=best_hidden, activation_fn=best_activation, l2=l2_weight_decay, epochs=n_epochs, n_features=n_features, random_seed=123)\n",
        "    \n",
        "    print('======== L2-norm regularization of weights : %f ========' % l2_weight_decay)\n",
        "    mlp_l2.train(training_set=(X_train_centered, y_train), validation_set=(X_valid_centered, y_valid))\n",
        "    \n",
        "    val_accs_2.append(mlp_l2.valid_acc_)\n",
        "    cost_accs_2.append(mlp_l2.train_loss_)\n",
        "    pred_accs_2.append(mlp_l2.predict(X_test_centered))\n",
        "    \n",
        "min_index = cost_accs_2.index(min(cost_accs_2))\n",
        "best_weight = l2_weight_decays[min_index]\n",
        "preds2 = pred_accs_2[min_index]\n",
        "print('\\n** best L2-norm regularization of weights: %f' % best_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIgzNLlQ9w5r",
        "colab_type": "text"
      },
      "source": [
        "    (2c) Evaluate the performance of (2b) for a few options of mini-batch sizes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6plt0xhg9w5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## a few options of mini-batch sizes\n",
        "batch_sizes = [128, 32, 16, 4]\n",
        "val_accs_3 = []\n",
        "cost_accs_3 = []\n",
        "pred_accs_3 = []\n",
        "for n_batch_size in batch_sizes:\n",
        "    mlp_batch = MLP(n_hidden=best_hidden, activation_fn=best_activation, l2=best_weight, batch_size=n_batch_size, epochs=n_epochs, n_features=n_features, random_seed=123)\n",
        "    \n",
        "    print('======== mini-batch sizes : %d ========' % n_batch_size)\n",
        "    mlp_batch.train(training_set=(X_train_centered, y_train), validation_set=(X_valid_centered, y_valid))\n",
        "    \n",
        "    val_accs_3.append(mlp_batch.valid_acc_)\n",
        "    cost_accs_3.append(mlp_batch.train_loss_)\n",
        "    pred_accs_3.append(mlp_batch.predict(X_test_centered))\n",
        "    \n",
        "min_index = cost_accs_3.index(min(cost_accs_3))\n",
        "best_batch = batch_sizes[min_index]\n",
        "preds3 = pred_accs_3[min_index]\n",
        "print('\\n** best mini-batch sizes: %d' % best_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EcS2tJa9w5v",
        "colab_type": "text"
      },
      "source": [
        "    (2d) Commonly for the results for (2a) - (2c), discuss the results such as by presenting convergence curves of cost and/or accuracy and by exemplifying the misclassified digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbbeA8Ig9w5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_misclassified(X, y, misclassified, predicted):\n",
        "    fig = plt.figure('misclassified digits', figsize=(10, 10))    \n",
        "    for i in range(10):\n",
        "        subplot = fig.add_subplot(2, 5, i+1)            \n",
        "        subplot.set_xticks([])\n",
        "        subplot.set_yticks([])            \n",
        "        subplot.set_title('ground truth : %d\\npredicted : %d' % (y[misclassified[i]], predicted[misclassified[i]]))            \n",
        "        subplot.imshow(X[misclassified[i], :].reshape((28, 28)), cmap=plt.cm.gray_r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGcR8XO-9w5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## a few choices of hidden nodes per hidden layer\n",
        "plt.plot(hidden_nodes, val_accs_0, label='acc per hidden nodes')\n",
        "plt.xlabel('hidden_size')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "print('Test Accuracy(hidden nodes): %.2f%%' % (100*np.sum(y_test == preds0)/len(y_test)))\n",
        "\n",
        "misclassified = np.where(preds0 != y_test)[0]\n",
        "plot_misclassified(X_test_centered, y_test, misclassified, preds0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtfSWJDp9w51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzewMlEP9w52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## a few choices of activation functions\n",
        "plt.plot([0, 1, 2], val_accs_1, label='acc per activation functions')\n",
        "plt.xlabel('activation functions (0:sigmoid, 1:tanh, 2:relu)')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "print('Test Accuracy(hidden nodes+activation): %.2f%%' % (100*np.sum(y_test == preds1)/len(y_test)))\n",
        "\n",
        "misclassified = np.where(preds1 != y_test)[0]\n",
        "plot_misclassified(X_test_centered, y_test, misclassified, preds1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_r_HyiV9w55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6LkSo29w56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## a few choices of L2-norm regularization parameters\n",
        "plt.plot(l2_weight_decays, val_accs_2, label='acc per L2-norm regularization weight')\n",
        "plt.xlabel('L2-norm regularization weight')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "print('Test Accuracy(hidden nodes+activation+L2-norm): %.2f%%' % (100*np.sum(y_test == preds2)/len(y_test)))\n",
        "\n",
        "misclassified = np.where(preds2 != y_test)[0]\n",
        "plot_misclassified(X_test_centered, y_test, misclassified, preds2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiUzHtzK9w58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffWlRGf9w5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## a few options of mini-batch sizes\n",
        "plt.plot(batch_sizes, val_accs_3, label='acc per mini-batch sizes')\n",
        "plt.xlabel('mini-batch sizes')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "print('Test Accuracy(hidden nodes+activation+L2-norm+mini-batch): %.2f%%' % (100*np.sum(y_test == preds3)/len(y_test)))\n",
        "\n",
        "misclassified = np.where(preds3 != y_test)[0]\n",
        "plot_misclassified(X_test_centered, y_test, misclassified, preds3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLR29dQl9w6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SczxLqFQ9w6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}